# app.py
import os
import json
import time
import base64
from datetime import datetime
from dotenv import load_dotenv

# ğŸ›¡ï¸ proxy ç’°å¢ƒå¤‰æ•°ã®å‰Šé™¤
for var in ["HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy"]:
    os.environ.pop(var, None)

from flask import Flask, request, jsonify
from flask_cors import CORS
from google.oauth2 import service_account
from googleapiclient.discovery import build
from openai import OpenAI
import faiss
import numpy as np
from product_film_matcher import ProductFilmMatcher
from query_expander import expand_query
from expand_reserve_query import expand_reserve_query

# â‘  å…±é€šè¨­å®šï¼ˆã“ã“ã«ãƒ‘ã‚¹ã‚’å®šç¾©ï¼‰
EMBED_MODEL = "text-embedding-3-small"
VECTOR_PATH = "data/vector_data.npy"
INDEX_PATH = "data/index.faiss"
RESERVE_VECTOR_PATH = "data/reserve_vector_data.npy"
RESERVE_INDEX_PATH = "data/reserve_index.faiss"

load_dotenv()

app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

session_histories = {}
HISTORY_TTL = 1800

def get_session_history(session_id):
    now = time.time()
    session = session_histories.get(session_id)
    if not session or now - session["last_active"] > HISTORY_TTL:
        session_histories[session_id] = {"last_active": now, "history": []}
    else:
        session_histories[session_id]["last_active"] = now
    return session_histories[session_id]["history"]

def add_to_session_history(session_id, role, content):
    history = get_session_history(session_id)
    history.append({"role": role, "content": content})
    if len(history) > 10:
        history[:] = history[-10:]

# é€šå¸¸ç”¨
with open("data/faq.json", encoding="utf-8") as f:
    faq_items = json.load(f)
faq_questions = [item["question"] for item in faq_items]
faq_answers = [item["answer"] for item in faq_items]

with open("data/knowledge.json", encoding="utf-8") as f:
    knowledge_dict = json.load(f)
knowledge_contents = [
    f"{category}ï¼š{text}" for category, texts in knowledge_dict.items() for text in texts
]

metadata_note = ""
metadata_path = "data/metadata.json"
if os.path.exists(metadata_path):
    with open(metadata_path, encoding="utf-8") as f:
        metadata = json.load(f)
        metadata_note = f"{metadata.get('title', '')} (ç¨®é¡: {metadata.get('type', '')}, å„ªå…ˆåº¦: {metadata.get('priority', '')})"

search_corpus = faq_questions + knowledge_contents
source_flags = ["faq"] * len(faq_questions) + ["knowledge"] * len(knowledge_contents)

# âœ… äºˆç´„ã‚·ã‚¹ãƒ†ãƒ ç”¨ FAQ ã®èª­ã¿è¾¼ã¿
with open("data/reserve_faq.json", encoding="utf-8") as f:
    reserve_faq_items = json.load(f)
reserve_faq_questions = [item["question"] for item in reserve_faq_items]
reserve_faq_answers = [item["answer"] for item in reserve_faq_items]

# âœ… äºˆç´„ã‚·ã‚¹ãƒ†ãƒ ç”¨ Knowledge ã®èª­ã¿è¾¼ã¿
with open("data/reserve_knowledge.json", encoding="utf-8") as f:
    reserve_knowledge_dict = json.load(f)
reserve_knowledge_contents = [
    f"{category}ï¼š{text}" for category, texts in reserve_knowledge_dict.items() for text in texts
]

# âœ… äºˆç´„ã‚·ã‚¹ãƒ†ãƒ ç”¨ æ¤œç´¢å¯¾è±¡ã¨ãƒ•ãƒ©ã‚°
reserve_search_corpus = reserve_faq_questions + reserve_knowledge_contents
reserve_source_flags = ["faq"] * len(reserve_faq_questions) + ["knowledge"] * len(reserve_knowledge_contents)

# âœ… é€šå¸¸ç”¨ FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ç”Ÿæˆ
if os.path.exists(VECTOR_PATH) and os.path.exists(INDEX_PATH):
    vector_data = np.load(VECTOR_PATH)
    index = faiss.read_index(INDEX_PATH)
else:
    vector_data = np.array([get_embedding(text) for text in search_corpus], dtype="float32")
    index = faiss.IndexFlatL2(vector_data.shape[1])
    index.add(vector_data)
    np.save(VECTOR_PATH, vector_data)
    faiss.write_index(index, INDEX_PATH)

# âœ… äºˆç´„ã‚·ã‚¹ãƒ†ãƒ ç”¨ FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ç”Ÿæˆ
if os.path.exists(RESERVE_VECTOR_PATH) and os.path.exists(RESERVE_INDEX_PATH):
    reserve_vector_data = np.load(RESERVE_VECTOR_PATH)
    reserve_index = faiss.read_index(RESERVE_INDEX_PATH)
else:
    reserve_vector_data = np.array([get_embedding(text) for text in reserve_search_corpus], dtype="float32")
    reserve_index = faiss.IndexFlatL2(reserve_vector_data.shape[1])
    reserve_index.add(reserve_vector_data)
    np.save(RESERVE_VECTOR_PATH, reserve_vector_data)
    faiss.write_index(reserve_index, RESERVE_INDEX_PATH)

# --- äºˆç´„å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
with open("data/reserve_faq.json", "r", encoding="utf-8") as f:
    reserve_faq_list = json.load(f)

with open("data/reserve_knowledge.json", "r", encoding="utf-8") as f:
    reserve_knowledge_dict = json.load(f)

reserve_knowledge_texts = [
    f"{category}ï¼š{text}"
    for category, texts in reserve_knowledge_dict.items()
    for text in texts
]

reserve_corpus = [
    f"{item['question']} {item['answer']}" for item in reserve_faq_list
] + reserve_knowledge_texts

reserve_index = faiss.read_index("data/reserve_index.faiss")
# --- ã“ã“ã¾ã§è¿½åŠ  ---

def get_embedding(text):
    if not text or not text.strip():
        raise ValueError("ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã«ã¯åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“")
    try:
        response = client.embeddings.create(
            model=EMBED_MODEL,
            input=[text]
        )
        if not response.data or not response.data[0].embedding:
            raise ValueError("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return np.array(response.data[0].embedding, dtype="float32")
    except Exception as e:
        print("âŒ Embedding error:", e)
        raise

# ğŸ”½ ã“ã“ã«äºˆç´„ç”¨æ¤œç´¢é–¢æ•°ã‚’è¿½åŠ 

def search_reserve_knowledge(user_q, k=3):
    query_vector = get_embedding(user_q).astype("float32").reshape(1, -1)
    scores, indices = reserve_index.search(query_vector, k)
    hits = [reserve_corpus[i] for i in indices[0] if i < len(reserve_corpus)]
    return hits

# ğŸ”½ ã“ã“ã«åˆ¤å®šé–¢æ•°ã‚’è¿½åŠ 
def is_reserve_query(user_q):
    keywords = ["äºˆç´„", "ç´æœŸ", "è£½é€ æ—¥", "ç´å“", "ã‚¢ã‚¯ã‚»ã‚¹", "ID", "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", "ãƒ­ã‚°ã‚¤ãƒ³"]
    return any(kw in user_q for kw in keywords)

if os.path.exists(VECTOR_PATH) and os.path.exists(INDEX_PATH):
    vector_data = np.load(VECTOR_PATH)
    index = faiss.read_index(INDEX_PATH)
else:
    vector_data = np.array([get_embedding(text) for text in search_corpus], dtype="float32")
    index = faiss.IndexFlatL2(vector_data.shape[1])
    index.add(vector_data)
    np.save(VECTOR_PATH, vector_data)
    faiss.write_index(index, INDEX_PATH)

SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
UNANSWERED_SHEET = "faq_suggestions_reserve"
FEEDBACK_SHEET = "feedback_log_reserve"
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

credentials_info = json.loads(base64.b64decode(os.environ["GOOGLE_CREDENTIALS"]).decode("utf-8"))
credentials = service_account.Credentials.from_service_account_info(credentials_info, scopes=SCOPES)
sheet_service = build("sheets", "v4", credentials=credentials).spreadsheets()

pf_matcher = ProductFilmMatcher("data/product_film_color_matrix.json")

with open("system_prompt.txt", encoding="utf-8") as f:
    base_prompt = f.read()

def infer_response_mode(question):
    q_len = len(question)
    if q_len < 30:
        return "short"
    elif q_len > 100:
        return "long"
    else:
        return "default"
    
CHAT_LOG_SHEET = "chat_logs_reserve"

# âœ… ãƒ­ã‚°è¨˜éŒ²é–¢æ•°ï¼ˆã“ã“ã«è¿½åŠ ï¼‰
def log_chat_history(user_q, answer, source_type, is_unanswered):
    try:
        sheet_service.values().append(
            spreadsheetId=SPREADSHEET_ID,
            range=f"{CHAT_LOG_SHEET}!A2:E",
            valueInputOption="RAW",
            body={"values": [[
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                user_q.strip(),
                answer.strip(),
                source_type,
                str(is_unanswered).lower()
            ]]}
        ).execute()
    except Exception as e:
        print("âŒ ãƒ­ã‚°å‡ºåŠ›å¤±æ•—:", e)

GREETING_PATTERNS = ["ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã°ã‚“ã¯", "ãŠã¯ã‚ˆã†", "ã¯ã˜ã‚ã¾ã—ã¦", "å®œã—ããŠé¡˜ã„ã—ã¾ã™", "ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™"]

@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.get_json()
        user_q = data.get("question", "").strip()
        session_id = data.get("session_id", "default")

        if not user_q:
            return jsonify({"error": "è³ªå•ãŒã‚ã‚Šã¾ã›ã‚“"}), 400

        if any(greet in user_q for greet in GREETING_PATTERNS):
            reply = "ã“ã‚“ã«ã¡ã¯ï¼ã”è³ªå•ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãã€‚"
            add_to_session_history(session_id, "assistant", reply)
            return jsonify({
                "response": reply,
                "original_question": user_q,
                "expanded_question": user_q
            })

        add_to_session_history(session_id, "user", user_q)
        session_history = get_session_history(session_id)

        # === ã‚¯ã‚¨ãƒªã®ç¨®é¡ã«å¿œã˜ã¦ãƒªãƒ©ã‚¤ãƒˆé–¢æ•°ã‚’è‡ªå‹•é¸æŠ + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¯¾è±¡ã‚’æ±ºå®š ===
        lower_q = user_q.lower()
        if any(x in lower_q for x in ["äºˆç´„", "ãƒ­ã‚°ã‚¤ãƒ³", "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", "ç™»éŒ²"]):
            expanded_q = expand_reserve_query(user_q, session_history)
            use_reserve = True
        else:
            expanded_q = expand_query(user_q, session_history)
            use_reserve = False

        q_vector = get_embedding(expanded_q)

        if use_reserve:
            D, I = reserve_index.search(np.array([q_vector]), k=7)
            search_source_flags = reserve_source_flags
            search_faq_questions = reserve_faq_questions
            search_faq_answers = reserve_faq_answers
            search_knowledge_contents = reserve_knowledge_contents
        else:
            D, I = index.search(np.array([q_vector]), k=7)
            search_source_flags = source_flags
            search_faq_questions = faq_questions
            search_faq_answers = faq_answers
            search_knowledge_contents = knowledge_contents

        D, I = index.search(np.array([q_vector]), k=7)
        if I.shape[1] == 0:
            raise ValueError("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        faq_context = []
        reference_context = []

        for idx in I[0]:
            if idx >= len(search_source_flags):
                continue
            src = search_source_flags[idx]
            if src == "faq":
                q = search_faq_questions[idx]
                a = search_faq_answers[idx]
                faq_context.append(f"Q: {q}\nA: {a}")
            elif src == "knowledge":
                ref_idx = idx - len(search_faq_questions)
                if ref_idx < len(search_knowledge_contents):
                    reference_context.append(f"ã€å‚è€ƒçŸ¥è­˜ã€‘{search_knowledge_contents[ref_idx]}")

        film_match_data = pf_matcher.match(user_q, session_history)
        film_info_text = pf_matcher.format_match_info(film_match_data)
        if film_info_text:
            reference_context.insert(0, film_info_text)

        if metadata_note:
            reference_context.append(f"ã€å‚è€ƒãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã€‘{metadata_note}")

        if not faq_context and not reference_context and not film_info_text.strip():
            answer = (
                "å½“ç¤¾ã¯ã‚³ãƒ¼ãƒ’ãƒ¼è£½å“ã®å§”è¨—åŠ å·¥ã‚’å°‚é–€ã¨ã™ã‚‹ä¼šç¤¾ã§ã™ã€‚"
                "æã‚Œå…¥ã‚Šã¾ã™ãŒã€ã”è³ªå•å†…å®¹ãŒå½“ç¤¾æ¥­å‹™ã¨ç›´æ¥é–¢é€£ã®ã‚ã‚‹å†…å®¹ã‹ã©ã†ã‹ã‚’ã”ç¢ºèªã®ã†ãˆã€"
                "æ”¹ã‚ã¦ãŠå°‹ã­ã„ãŸã ã‘ã¾ã™ã¨å¹¸ã„ã§ã™ã€‚\n\n"
                "ã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€å½“ç¤¾ã®ã€ãŠå•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ã€‘ã‚ˆã‚Šã”é€£çµ¡ãã ã•ã„ã€‚"
            )
            add_to_session_history(session_id, "assistant", answer)
            return jsonify({
                "response": answer,
                "original_question": user_q,
                "expanded_question": expanded_q
            })

        faq_part = "\n\n".join(faq_context[:3]) if faq_context else "è©²å½“ã™ã‚‹FAQã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        ref_texts = [text for text in reference_context if "è£½å“ãƒ•ã‚£ãƒ«ãƒ ãƒ»ã‚«ãƒ©ãƒ¼æƒ…å ±" in text]
        other_refs = [text for text in reference_context if "è£½å“ãƒ•ã‚£ãƒ«ãƒ ãƒ»ã‚«ãƒ©ãƒ¼æƒ…å ±" not in text][:2]
        ref_part = "\n".join(ref_texts + other_refs)

        mode = infer_response_mode(user_q)

        prompt = f"""ä»¥ä¸‹ã¯å½“ç¤¾ã®FAQãŠã‚ˆã³å‚è€ƒæƒ…å ±ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«è£½é€ å…ƒã®ç«‹å ´ã§ã”å›ç­”ãã ã•ã„ã€‚

ã€FAQã€‘
{faq_part}

ã€å‚è€ƒæƒ…å ±ã€‘
{ref_part}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_q}
å›ç­”ï¼š"""

        system_prompt = base_prompt
        if mode == "short":
            system_prompt += "\n\nå¯èƒ½ãªé™ã‚Šç°¡æ½”ã‹ã¤è¦ç‚¹ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        elif mode == "long":
            system_prompt += "\n\nè©³ç´°ãªèª¬æ˜ã‚„å…·ä½“ä¾‹ã‚’å«ã‚ã¦ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"

        completion = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        answer = completion.choices[0].message.content.strip()

        if "ç”³ã—è¨³" in answer or "æã‚Œå…¥ã‚Šã¾ã™ãŒ" in answer or "ã‚¨ãƒ©ãƒ¼" in answer:
            sheet_service.values().append(
                spreadsheetId=SPREADSHEET_ID,
                range=f"{UNANSWERED_SHEET}!A2:D",
                valueInputOption="RAW",
                body={"values": [[datetime.now().strftime("%Y-%m-%d %H:%M:%S"), user_q, "æœªå›ç­”", 1]]}
            ).execute()

        add_to_session_history(session_id, "assistant", answer)

        # âœ… å›ç­”ã‚½ãƒ¼ã‚¹ãƒ»æœªå›ç­”åˆ¤å®šãƒ»ãƒ­ã‚°å‡ºåŠ›ã‚’ã“ã“ã«è¿½åŠ 
        if use_reserve:
            source_type = "reserve_faq" if "Q:" in faq_part else "reserve_knowledge"
        else:
            source_type = "faq" if "Q:" in faq_part else "knowledge"

        is_unanswered = any(phrase in answer for phrase in ["ç”³ã—è¨³", "æã‚Œå…¥ã‚Šã¾ã™ãŒ", "ã‚¨ãƒ©ãƒ¼"])
        log_chat_history(user_q, answer, source_type, is_unanswered)

        return jsonify({
            "response": answer,
            "original_question": user_q,
            "expanded_question": expanded_q
        })

    except Exception as e:
        print("[ERROR in /chat]:", e)
        return jsonify({
            "response": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "error": str(e)
        }), 500

@app.route("/feedback", methods=["POST"])
def feedback():
    data = request.get_json()
    question = data.get("question")
    answer = data.get("answer")
    feedback_value = data.get("feedback")
    reason = data.get("reason", "")

    if not all([question, answer, feedback_value]):
        return jsonify({"error": "ä¸å®Œå…¨ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§ã™"}), 400

    sheet_service.values().append(
        spreadsheetId=SPREADSHEET_ID,
        range=f"{FEEDBACK_SHEET}!A2:E",
        valueInputOption="RAW",
        body={"values": [[datetime.now().strftime("%Y-%m-%d %H:%M:%S"), question, answer, feedback_value, reason]]}
    ).execute()

    return jsonify({"status": "success"})

@app.route("/", methods=["GET"])
def home():
    return "Chatbot API is running."

if __name__ == "__main__":
    app.run(debug=True)